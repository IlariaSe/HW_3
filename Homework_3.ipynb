{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import enchant\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Airbnb_Texas_Rentals.csv')\n",
    "\n",
    "for i in range(df.index.max()+1):\n",
    "    file=open('test/doc_'+str(i+1)+\".tsv\",'w',newline='')\n",
    "    for j in range(9):\n",
    "        file.write('%s\\t'% df.iloc[i,j])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordNorm(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer() #with or without english?\n",
    "\n",
    "    Normalized = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    word_list = tokenizer.tokenize(text.lower())\n",
    "    Remove_stopwords = [lemmatizer.lemmatize(stemmer.stem(word)) for word in word_list if word not in stopwords.words('english')]\n",
    "    \n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    for word in Remove_stopwords:\n",
    "        if not word.isdigit() and len(word)>1 and d.check(word):#remove the punctuation\n",
    "            Normalized.append(word)\n",
    "    return Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readall_documents(path):\n",
    "    words = []\n",
    "    for file in glob.glob(path):\n",
    "        df = pd.read_table(file,sep='\\t',names=['description','title'])\n",
    "        desc = wordNorm(str(list(df['description'])))\n",
    "        title = wordNorm(str(list(df['title'])))\n",
    "        words.append(desc)\n",
    "        words.append(title)\n",
    "    words = list(it.chain(*words))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all the files inside test folder. Test folder contains all the tsv files\n",
    "Allword = readall_documents('data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateVocabulary(allwords):\n",
    "    Vocabulary = {}\n",
    "    wordList = sorted(list(set(allwords)))\n",
    "    for ID, element in enumerate(wordList):\n",
    "        Vocabulary.update({element : ID})\n",
    "    return Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = CreateVocabulary(Allword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, text):\n",
    "    count = 0\n",
    "    if len(text) <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        for ter in text:\n",
    "            if ter == term:\n",
    "                count += 1\n",
    "        return count / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build invertedIndex\n",
    "def invertedIndex(vocabulary,path):\n",
    "    # invertedIndex = {termID : (doc, TF)}\n",
    "    invertedIndex = {}\n",
    "    counter = 0\n",
    "    for file in glob.glob(path):\n",
    "        df = pd.read_table(file,sep='\\t',names=['average_rate_per_night','bedrooms_count',\n",
    "                                                'city','date_of_listing','description','latitude',\n",
    "                                                'longitude','title','url'])\n",
    "        \n",
    "        desc = wordNorm(str(list(df['description'])))\n",
    "        title = wordNorm(str(list(df['title'])))\n",
    "        for word in vocabulary:\n",
    "            tf = term_freq(word, desc+title)\n",
    "            if(tf>0):\n",
    "                if(vocabulary[word] in invertedIndex.keys()):\n",
    "                    invertedIndex[vocabulary[word]].append(file)\n",
    "                else:\n",
    "                    invertedIndex[vocabulary[word]] = [file]\n",
    "\n",
    "    return invertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "invIndex = invertedIndex(voc,'test/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save our invertedIndex file for future use.\n",
    "with open('inverted.json', 'w') as fp:\n",
    "    json.dump(invIndex, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"lovely land\" #Accept the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnorm = wordNorm(query) #Normalize the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find a document that contains all the words in the qnorm\n",
    "inter = set()\n",
    "for word in qnorm:\n",
    "    if word in voc:\n",
    "        if(len(inter) >0):\n",
    "            inter = inter.intersection(set(invIndex[voc[word]]))\n",
    "        else:\n",
    "            inter = inter.union(invIndex[voc[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title\n",
      "=========:\n",
      "['Texas Starlight Ranch']\n",
      "\n",
      " Description\n",
      "=========:\n",
      "['Texas Starlight Ranch sits on 18 acres of land and is also a nature preserve. My place is close to restaurants and dining and family-friendly activities. You’ll love my place because of the ambiance and the outdoors space. My place is good for couples, solo adventurers, business travelers, families (with kids), and big groups. Texas Starlight Ranch is also great for Holiday Parties for Thanksgiving, Christmas and New Years.']\n",
      "\n",
      " City\n",
      "=========:\n",
      "['McKinney']\n",
      "\n",
      " URL\n",
      "=========:\n",
      "['https://www.airbnb.com/rooms/13113265?location=Anna%2C%20TX']\n",
      "Title\n",
      "=========:\n",
      "['Beautiful Texas Villa For Family/Weekend Getaways']\n",
      "\n",
      " Description\n",
      "=========:\n",
      "['Experience the quiet and relaxation of the Texas countryside while gathering with friends and/or family. Lovingly named Mi Sueno (“My Dream”) this home is ideal for small to large overnight gatherings of 10-20 people. Mi Sueno is located just outside of the small town of Sealy, TX upon 40 acres of rolling Texas land. The villa itself has 5 bedroom, 4.5 bathrooms, gourmet kitchen with Viking appliances, 10 gathering spaces as well as sleeping rooms for up to 16 people.']\n",
      "\n",
      " City\n",
      "=========:\n",
      "['Sealy']\n",
      "\n",
      " URL\n",
      "=========:\n",
      "['https://www.airbnb.com/rooms/18794108?location=Chappell%20Hill%2C%20TX']\n",
      "Title\n",
      "=========:\n",
      "['New Arrival Haven - comfy and close']\n",
      "\n",
      " Description\n",
      "=========:\n",
      "['Our place is a perfectly situated landing pad for new arrivals. We are close to almost every corner of the city, and we love giving new Austinites a warm welcome. We are totally a smoke-free home.']\n",
      "\n",
      " City\n",
      "=========:\n",
      "['Austin']\n",
      "\n",
      " URL\n",
      "=========:\n",
      "['https://www.airbnb.com/rooms/13676088?location=Austin%2C%20TX']\n"
     ]
    }
   ],
   "source": [
    "for file in inter:\n",
    "    df = pd.read_table(file,sep='\\t',names=['index','average_rate_per_night','bedrooms_count',\n",
    "                                                'city','date_of_listing','description','latitude',\n",
    "                                                'longitude','title','url'])\n",
    "    print(\"Title\\n=========:\")\n",
    "    print(list(df['title']))\n",
    "    print(\"\\n Description\\n=========:\")\n",
    "    print(list(df['description']))\n",
    "    print(\"\\n City\\n=========:\")\n",
    "    print(list(df['city']))\n",
    "    print(\"\\n URL\\n=========:\")\n",
    "    print(list(df['url']))\n",
    "    df1= df[['title','description','city','url']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
